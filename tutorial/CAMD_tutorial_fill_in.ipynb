{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential learning with CAMD - fill in the blanks\n",
    "\n",
    "**Objective:** imagine you want to design a procedure in order to discover materials which are resistant to deformation.  Such efforts have been the subject of at least one [recent paper](https://pubs.acs.org/doi/10.1021/jacs.8b02717) which used machine learning to predict and verify a new superhard material, (ReWC0.8).  How would we design a method that iterated on each of its past experiments in order to improve itself?  In this notebook, we cover methods supported by [CAMD](https://s3-eu-west-1.amazonaws.com/itempdf74155353254prod/11860104/Autonomous_Intelligent_Agents_for_Accelerated_Materials_Discovery_v1.pdf), a software package designed to assist scientists with sequential learning, that will reveal how different methods of decision making and feedback perform on a known dataset of materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "The dataset we'll be using in the tutorial will be the elastic tensor dataset from Maarten de Jong's 2015 paper, [Charting the complete elastic properties of inorganic crystalline compounds](https://www.nature.com/articles/sdata20159).  We'll be using the [MatMiner](https://hackingmaterials.lbl.gov/matminer/) API to fetch the data, which we've written a function for in the helper code pre-installed on your SageMaker instance.  In order to make our data compatible with some machine learning functionality later on, we'll be featurizing it, also using MatMiner.  Lastly, we'll lay the groundwork for the simulation of our sequential learning procedure by separating the featurized data into **seed_data** which we will assume we have full knowledge of a-priori, and **candidate_data** which we will assume we know nothing about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "from hackathon.helper import load_tutorial_data\n",
    "data = load_tutorial_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data and inspect again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the lowest five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate magpie features\n",
    "from matminer.featurizers.composition import ElementProperty\n",
    "from pymatgen import Composition\n",
    "\n",
    "# Featurize dataframe here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect features of top candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we partition the data by choosing every other member of our known data for the seed data and the remainder for our candidate data.  Note that this partitioning can have a **significant** impact on how the sequential learning procedure progresses.  As an exercise, you might try seeing how the notebook compares if you use the alternative commented option where the seed is the bottom half of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition data into seed and candidate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"answers\" from candidate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: choose bottom half\n",
    "# half = int(len(featurized_data) / 2)\n",
    "# k_seed_data = featurized_data.iloc[half:]\n",
    "# k_candidate_data = featurized_data.iloc[:half]\n",
    "# k_candidate_data.drop(['bulk_modulus', 'shear_modulus'], axis=1)\n",
    "\n",
    "# Alternative: choose randomly\n",
    "# half = int(len(featurized_data) / 2)\n",
    "# k_seed_data = featurized_data.sample(half)\n",
    "# k_candidate_data = featurized_data.loc[~k_seed_data]\n",
    "# k_candidate_data.drop(['bulk_modulus', 'shear_modulus'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to ensure no overlap\n",
    "assert not set(k_seed_data.index).intersection(k_candidate_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "In CAMD, Hypothesis *Agents* are python objects which select candidates on which to perform experiments.  Almost all of the \"AI\" components, including ML algorithms, simpler regression, and even random selection, within CAMD are contained in logic implemented within Agents.  \n",
    "\n",
    "\n",
    "To implement a CAMD-compatible Agent, we use the *HypothesisAgent* abstract class, which basically will issue an error if we don't fulfill all of the things we need to in order to ensure that our Agent is compatible with the sequential learning process implemented in a CAMD *Campaign* (more on Campaigns later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camd.agent.base import HypothesisAgent\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def get_magpie_features(dataframe):\n",
    "    \"\"\"Helper function to get features of dataframe\"\"\"\n",
    "    magpie_columns = [column for column in dataframe \n",
    "                      if column.startswith(\"MagpieData\")]\n",
    "    return dataframe[magpie_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearHardnessAgent(HypothesisAgent):\n",
    "    def get_hypotheses(self, candidate_data, seed_data):\n",
    "        # Fit on known data\n",
    "        \n",
    "        # Predict unknown data\n",
    "        \n",
    "        # Pick top 5 candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with the Agent a bit to see what it recommends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LinearHardnessAgent's get hypotheses with seed/candidate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Use a random forest regression\n",
    "* How do its selections from our dataset differ?\n",
    "* Try varying the parameters of the regressor - n_estimators, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference, here is how you invoke a random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_regressor = RandomForestRegressor()\n",
    "help(RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement rf agent\n",
    "class RFHardnessAgent(HypothesisAgent):\n",
    "    def get_hypotheses(self, candidate_data, seed_data):\n",
    "        # Fit on known data\n",
    "        \n",
    "        # Predict unknown data\n",
    "        \n",
    "        # Pick top 5 candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test rf agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to top ten candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "In CAMD, *Experiments* are objects that are used to generate new data corresponding to the output of the *Agent.get_hypotheses* method.  In other words, *Agents* pick the candidates on which you want to do experiments, and *Experiments* actually do those experiments.  As of today, only two experiments are implemented in CAMD, one of which is a AWS-based density functional theory computation of an input crystal structure.  The other, which we'll demonstrate below, is an *after-the-fact sampler*, which basically fetches the result of an experiment we already did that corresponds to the input.\n",
    "\n",
    "Why is the ATFSampler useful?  We'll discuss simulation in more detail in a bit, but let's just say we use the ATFSampler to help us evaluate the performance of an Agent when we're trying to pick which agent is the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ATF Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke ATF agent with featurized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that experiments are *stateful* meaning that their state is explicitly controlled by the user using the `submit` method.  When a new set of experiments are submitted, the previous experiments are appended to an internal history attribute and the new ones are set as the current experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit hypotheses and get results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzers\n",
    "\n",
    "**Analyzers** are a bit tricky to explain because they're not necessary for every sequential learning process.  We're not going to spend much time on them here other than to say that, after you've performed an experiment, sometimes you want to postprocess the data in order to summarize the results of the current iteration and to augment the **seed data** which is being used to provide the **Agent** with the information it needs to make its next decision on which candidates to select for further experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camd.analysis import AnalyzerBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BulkModulusAnalyzer(AnalyzerBase):\n",
    "    def analyze(self, new_experimental_results, seed_data):\n",
    "        # Create new seed by concatenating old seed and new experiments\n",
    "        new_seed = pd.concat(\n",
    "            [seed_data, new_experimental_results],\n",
    "        axis=0)\n",
    "        \n",
    "        # Do a few stats on the aggregated results\n",
    "        # Mean new bulk modulus\n",
    "        average_new_bulk_modulus = new_experimental_results.bulk_modulus.mean()\n",
    "        # Average cumulative bulk modulus\n",
    "        average_dataset_bulk_modulus = new_seed.bulk_modulus.mean()\n",
    "        # Average rank of new data\n",
    "        new_result_ranks = new_seed.bulk_modulus.rank(pct=True).loc[\n",
    "            new_experimental_results.index\n",
    "        ]\n",
    "        \n",
    "        # Construct a summary dataframe to return with the seed\n",
    "        summary = pd.DataFrame({\n",
    "            \"average_new_bulk_modulus\": [average_new_bulk_modulus],\n",
    "            \"average_dataset_bulk_modulus\": [average_dataset_bulk_modulus],\n",
    "            \"average_rank\": [new_result_ranks.mean()]\n",
    "        })\n",
    "        return summary, new_seed  # You must return both objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results with seed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data, Campaigns, and Simulations\n",
    "\n",
    "Now that we've got all of the building blocks in place, let's try putting everything together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from monty.os import cd\n",
    "from camd.campaigns.base import Campaign\n",
    "# Set up folders\n",
    "os.system('rm -rf test')\n",
    "os.system('mkdir -p test')\n",
    "# Reinitialize experiment to clear history\n",
    "k_atf_experiment = ATFSampler(dataframe=featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke, initialize, and run campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch aggregated history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect top of recent history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some highlighting\n",
    "k_candidate_data.style.apply(\n",
    "    lambda x: ['background: darkorange' \n",
    "               if (x.name in result_history.index)\n",
    "               else '' for i in x], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts\n",
    "There's a lot more that we can do to improve our postprocessing analysis of how well the campaign proceeded, but this should get you started.  A few exercises you might find interesting to try:\n",
    "\n",
    "* Test different regressors from scikit learn, this [documentation of their supervised learning methods](https://scikit-learn.org/stable/supervised_learning.html) points to many of them.\n",
    "* Test the agent on using multiple random seeds and determine the spread on discovery rate.\n",
    "* Develop an explore/exploit strategy where you choose some candidates from the regressor prediction and some randomly.\n",
    "* Try different datasets, see the datasets folder or the [matminer datasets documentation](https://hackingmaterials.lbl.gov/matminer/dataset_summary.html).\n",
    "* Try different featurizers, see [the matminer featurizer documentation](https://hackingmaterials.lbl.gov/matminer/featurizer_summary.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "* **Agent** - decision making object in camd, must implement `get_hypotheses` in order to work properly in the loop\n",
    "* **Experiment** - object which performs some action in order to determine unknowns about an input dataset\n",
    "* **Analyzer** - object which postprocesses experimental outputs and prior seed data in order to provide a new seed data\n",
    "* **seed_data** - Data which is \"known\" either before the start of a given **Campaign** or prior to any iteration.  Is used to inform the **Agent** of the data it should be using to make a decision about how to select from the **Candidate data**.\n",
    "* **candidate_data** - data which represents the information about the set of \"unknowns\" at a given point of time for a **Campaign**.\n",
    "* **Campaign** - the iterative procedure by which an **Agent** suggests experiments from the **candidate data**, the **Experiment** performs them, the **Analyzer** analyzes them and feeds a new **seed data** and set of **candidate data** back to the **Agent** to start a new iteration. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
